{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gzip\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "def load_mnist(path, kind, subset=None):\n",
    "    labels_path = os.path.join(path, '%s-labels-idx1-ubyte.gz'%kind)\n",
    "    images_path = os.path.join(path, '%s-images-idx3-ubyte.gz'%kind)\n",
    "\n",
    "    with gzip.open(labels_path, 'rb') as lbpath:\n",
    "        labels = np.frombuffer(lbpath.read(), dtype=np.uint8, offset=8)\n",
    "    with gzip.open(images_path, 'rb') as imgpath:\n",
    "        images = np.frombuffer(imgpath.read(), dtype=np.uint8, offset=16).reshape(len(labels), 784)\n",
    "    \n",
    "    if subset is not None:\n",
    "        selected_images, selected_labels = [], []\n",
    "        for label in range(10):\n",
    "            indices = np.where(labels == label)[0]\n",
    "            selected_indices = np.random.choice(indices, subset, replace=False)\n",
    "            selected_images.append(images[selected_indices])\n",
    "            selected_labels.append(labels[selected_indices])\n",
    "        images = np.concatenate(selected_images, axis=0)\n",
    "        labels = np.concatenate(selected_labels, axis=0)\n",
    "\n",
    "        paired = list(zip(images, labels))\n",
    "        random.shuffle(paired)\n",
    "        images, labels = zip(*paired)\n",
    "    \n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "# Data Preprocessing.\n",
    "X_train, y_train = load_mnist('./data/', kind='train')\n",
    "X_test, y_test = load_mnist('./data/', kind='t10k')\n",
    "\n",
    "X_train = X_train.reshape((X_train.shape[0], 28, 28, 1)).astype('float32') / 255\n",
    "X_test = X_test.reshape((X_test.shape[0], 28, 28, 1)).astype('float32') / 255\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Task1:**\n",
    "\n",
    "At the beginning, we need to build a very simple CNN model with the structure of\n",
    "1. A 2D convolutional layer with 16 filters with each size 3*3 (RELU activation function)\n",
    "2. A 2D maxpooling layer with 2*2 pooling window\n",
    "3. A flatten layer to convert 2D feature into 1D vector\n",
    "4. A fully connected layer using Softmax activation\n",
    "\n",
    "Remember that we are doing a image classification task, so we shall use categorical cross entropy function as the loss funtion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7644 - loss: 0.6751 - val_accuracy: 0.8577 - val_loss: 0.4005\n",
      "Epoch 2/2\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8697 - loss: 0.3749 - val_accuracy: 0.8762 - val_loss: 0.3440\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8784 - loss: 0.3532\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Input((28, 28, 1)))\n",
    "model.add(layers.Conv2D(16, (3, 3), activation = 'relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(10, activation = 'softmax'))\n",
    "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=2, batch_size=32, validation_split=0.1)  # Train the model\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)                        #  Test the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Task2:**\n",
    "\n",
    "Then we will take a challenge to implement a more complex CNN model to have a better classification performance. Here is a structure for your reference, or you can also design your own CNN model. The only requirement is to have a better performance than the simple CNN in Task 1 (a larger accuracy score on test set).\n",
    "\n",
    "The reference structure is devided into three parts:\n",
    "1. Primary Feature Extraction Part\n",
    "    1. A 2D convolutional layer with 32 filters with each size 3*3 (RELU activation function)\n",
    "    2. A normalization layer\n",
    "    3. A 2D maxpooling layer with 2*2 pooling window\n",
    "    4. A dropout layer (randomly drops 25% of units) (designed for preventing overfitting)\n",
    "2. Advanced Feature Extraction Part\n",
    "\n",
    "    This part is mostly similar to the previous section. The only difference is to use more filters (like 64) in convolutional layer to gain high-level features\n",
    "3. Classification Part\n",
    "    1. A flatten layer to convert 2D feature into 1D vector\n",
    "    2. A fully connected layer with 512 units using RELU to summerize high-dimensinal features\n",
    "    3. Another connected layer for Softmax classification\n",
    "\n",
    "Remember that we are doing a image classification task, so we shall use categorical cross entropy function as the loss funtion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 8ms/step - accuracy: 0.7948 - loss: 0.6297 - val_accuracy: 0.8527 - val_loss: 0.4147\n",
      "Epoch 2/2\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 9ms/step - accuracy: 0.8806 - loss: 0.3201 - val_accuracy: 0.8910 - val_loss: 0.2978\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8869 - loss: 0.3113\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Input((28, 28, 1)))\n",
    "\n",
    "# Extract Feature.\n",
    "model.add(layers.Conv2D(32, (3, 3), activation = 'relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Dropout(0.25)) \n",
    "\n",
    "# Advanced Feature Extract.\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))  # More filters for high-level features.\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Dropout(0.25))\n",
    "\n",
    "# Classify.\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(512, activation = 'relu'))\n",
    "model.add(layers.Dense(10, activation = 'softmax'))\n",
    "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=2, batch_size=32, validation_split=0.1)\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
